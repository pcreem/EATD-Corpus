import os
import sounddevice as sd
from scipy.io.wavfile import write
import whisper as openai_whisper
import torch
import numpy as np
from transformers import BertTokenizer, BertModel
from your_model import AudioTextEmotionModel, extract_audio_features, extract_text_features  # ç”¨ä½ çš„æ¨¡çµ„

# === åƒæ•¸è¨­å®š ===
SAMPLE_RATE = 16000
DURATION = 5  # éŒ„éŸ³ç§’æ•¸
MODEL_PATH = "model_weights.pth"
EMOTION_LABELS = {0: 'æ­£é¢', 1: 'ä¸­æ€§', 2: 'è² é¢'}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === è¼‰å…¥æ¨¡å‹ ===
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"Model file {MODEL_PATH} not found!")
    
emotion_model = AudioTextEmotionModel(audio_input_dim=180, text_input_dim=768, hidden_dim=128, output_dim=3)
emotion_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
emotion_model.to(device)
emotion_model.eval()

# === Whisper æ¨¡å‹è¼‰å…¥ ===
whisper_model = openai_whisper.load_model("base")

# === BERT tokenizer/model ===
tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")
bert_model = BertModel.from_pretrained("bert-base-chinese")

# === éŒ„éŸ³å‡½æ•¸ ===
def record_audio(filename="output.wav"):
    try:
        print(f"ğŸ¤ è«‹é–‹å§‹èªªè©±ï¼ˆ{DURATION} ç§’ï¼‰...")
        audio = sd.rec(int(SAMPLE_RATE * DURATION), samplerate=SAMPLE_RATE, channels=1)
        sd.wait()
        write(filename, SAMPLE_RATE, audio)
        print("âœ… éŒ„éŸ³å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ éŒ„éŸ³å¤±æ•—ï¼š{e}")

# === é æ¸¬æµç¨‹ ===
def predict_emotion(audio_path):
    # èªéŸ³è½‰æ–‡å­—
    result = whisper_model.transcribe(audio_path, language="zh")
    text = result["text"]
    print(f"ğŸ“ èªéŸ³è½‰æ–‡å­—çµæœï¼š{text}")

    # ç‰¹å¾µè™•ç†
    audio_feat = extract_audio_features(audio_path)
    text_feat = extract_text_features(text)
    if audio_feat is None or text_feat is None:
        raise ValueError("Audio or text features are empty!")

    # å¼µé‡è½‰æ›
    audio_tensor = torch.tensor(audio_feat, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)
    text_tensor = torch.tensor(text_feat, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)

    # é æ¸¬
    with torch.no_grad():
        output = emotion_model(audio_tensor, text_tensor)
        pred = torch.argmax(output, dim=1).item()
    
    print(f"ğŸ” é æ¸¬æƒ…ç·’ç‚ºï¼š{EMOTION_LABELS[pred]}")
    return text, EMOTION_LABELS[pred]

# === ä¸»åŸ·è¡Œ ===
if __name__ == "__main__":
    while True:
        input("ğŸ”˜ æŒ‰ä¸‹ Enter éµé–‹å§‹éŒ„éŸ³ï¼ˆæˆ– Ctrl+C é›¢é–‹ï¼‰")
        record_audio("user_input.wav")
        text, emotion = predict_emotion("user_input.wav")
        print(f"ğŸ¤– ChatBot å›æ‡‰ï¼šä½ èªªçš„æ˜¯ã€Œ{text}ã€ï¼Œæˆ‘è¦ºå¾—ä½ ç¾åœ¨çš„æƒ…ç·’æ˜¯ã€Œ{emotion}ã€ã€‚")
